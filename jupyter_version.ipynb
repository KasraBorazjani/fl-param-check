{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "\n",
    "\n",
    "from typing import Sequence\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import networkx as nx\n",
    "import pycxsimulator\n",
    "from parse_args import parse_arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to create a dataset given the labels it should contain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ds_from_labels(primary_label:int,\n",
    "                          secondary_labels:Sequence[int],\n",
    "                          label_indexes_list,\n",
    "                          total_ds_len:int,\n",
    "                          primary_label_fraction:float,\n",
    "                          original_ds:Dataset\n",
    "                          ):\n",
    "    \n",
    "    \n",
    "    # Find the primary label elements for dataset\n",
    "    primary_label_elements_num = int(primary_label_fraction * total_ds_len)\n",
    "    # print(\"num prim elems: \", primary_label_elements_num)\n",
    "    selected_primary_indexes = np.random.randint(low=0, high=len(label_indexes_list[primary_label]), size=primary_label_elements_num)\n",
    "    # print(selected_primary_indexes)\n",
    "    primary_label_idxs = label_indexes_list[primary_label][selected_primary_indexes]\n",
    "    label_indexes_list[primary_label] = label_indexes_list[primary_label][~np.isin(label_indexes_list[primary_label], primary_label_idxs)]\n",
    "    primary_label_subset = Subset(original_ds, primary_label_idxs)\n",
    "    # print(\"len subset prim: \", len(primary_label_subset))\n",
    "    \n",
    "    # Find the secondary label(s) elements for the dataset\n",
    "    secondary_label_elements_frac = ((1-primary_label_fraction)/len(secondary_labels))\n",
    "    secondary_label_elements_num = int(secondary_label_elements_frac*total_ds_len)\n",
    "    secondary_labels_subsets = []\n",
    "    for label in secondary_labels:\n",
    "        selected_label_indexes = np.random.randint(low=0, high=len(label_indexes_list[label]), size=secondary_label_elements_num)\n",
    "        selected_indexes = label_indexes_list[label][selected_label_indexes]\n",
    "        secondary_labels_subsets.append(Subset(original_ds, selected_indexes))\n",
    "        label_indexes_list[label] = label_indexes_list[label][~np.isin(label_indexes_list[label], selected_indexes)]\n",
    "    \n",
    "    # for item in secondary_labels_subsets:\n",
    "    #     print(len(item))\n",
    "\n",
    "    secondary_labels_subsets += [primary_label_subset]\n",
    "\n",
    "    return ConcatDataset(secondary_labels_subsets), label_indexes_list  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating pseudo-random uniform primary and secondary labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_enough_samples(label_idx_list, n_labels_needed, label_chosen):\n",
    "    if len(label_idx_list[label_chosen]) >= n_labels_needed:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def generate_random_label_set(label_idx_list, primary_dataset_len, secondary_dataset_len, num_secondaries):\n",
    "    selected_labels = []\n",
    "    while True:\n",
    "        sample_label = np.random.randint(low=0, high=10, dtype=int)\n",
    "        if has_enough_samples(label_idx_list, primary_dataset_len, sample_label):\n",
    "            selected_labels.append(sample_label)\n",
    "            break\n",
    "    \n",
    "    for i in range(num_secondaries):\n",
    "        while True:\n",
    "            sample_label = np.random.randint(low=0, high=10, dtype=int)\n",
    "            if (has_enough_samples(label_idx_list, primary_dataset_len, sample_label)) and (sample_label not in selected_labels):\n",
    "                selected_labels.append(sample_label)\n",
    "                break\n",
    "        \n",
    "    return selected_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a client's dataset by:\n",
    "1. Generating a pseudo-random set of labels for it to contain as primary and secondary labels\n",
    "2. Generating a random subset of the original MNIST dataset given the labels and the subset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_client_ds(original_ds, label_idx_list, total_ds_len, primary_label_fraction, num_secondaries):\n",
    "    primary_dataset_len = int(primary_label_fraction * total_ds_len)\n",
    "    secondary_dataset_len = int(((1-primary_label_fraction)/num_secondaries) * total_ds_len)\n",
    "    label_set = generate_random_label_set(label_idx_list, primary_dataset_len, secondary_dataset_len, num_secondaries)\n",
    "    client_ds, label_idx_list = create_ds_from_labels(label_set[0], label_set[1:], label_idx_list,\n",
    "                                total_ds_len, primary_label_fraction, original_ds)\n",
    "    \n",
    "    return client_ds, label_idx_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleModel(nn.Module):\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "\n",
    "        super(simpleModel, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.log_softmax(self.fc3(x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a client that\n",
    "- contains a dataset and a model\n",
    "- has a method defined for one epoch of local training\n",
    "- has a method to validate on a given dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_client():\n",
    "    def __init__(self, original_ds, client_id, label_idx_list, args):\n",
    "        self.client_id = client_id\n",
    "        self.dataset, label_idx_list = create_client_ds(original_ds, label_idx_list, args.total_ds_len, args.primary_label_fraction, args.num_secondaries)\n",
    "        self.dataloader = DataLoader(dataset=self.dataset, batch_size=args.batch_size, shuffle=args.shuffle_dataset)\n",
    "        self.model = simpleModel()\n",
    "        self.model.load_state_dict(torch.load(os.path.join(args.saved_model_path, 'initial_model_weights.pth')))\n",
    "        self.history = {\"train_acc\":[], \"train_loss\":[], \"val_acc\":[], \"val_loss\":[]}\n",
    "        self.sgd_per_round = args.sgd_per_round\n",
    "        self.device = args.device\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=0.01, momentum=0.5)\n",
    "        self.criterion = nn.NLLLoss()\n",
    "    \n",
    "    def train_one_round(self):\n",
    "        \n",
    "        self.model.train()\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        running_loss = 0\n",
    "        for idx, (data, labels) in enumerate(self.dataloader):\n",
    "\n",
    "            if idx > self.sgd_per_round:\n",
    "                break\n",
    "\n",
    "            data = data.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            output = self.model(data.view(data.shape[0], -1))\n",
    "            loss = self.criterion(output, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            train_correct += torch.sum(pred==labels).item()\n",
    "            train_total += labels.size(0)\n",
    "        \n",
    "        self.history['train_loss'].append(running_loss/len(self.dataloader))\n",
    "        self.history['train_acc'].append(100*train_correct/train_total)\n",
    "    \n",
    "    def validate_model(self, val_loader):\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            running_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for idx, (data, labels) in enumerate(val_loader):\n",
    "                data = data.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                output = self.model(data.view(data.shape[0], -1))\n",
    "                loss = self.criterion(output, labels)\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, pred = torch.max(output, dim=1)\n",
    "                correct += torch.sum(pred==labels).item()\n",
    "                total += labels.size(0)\n",
    "            \n",
    "            self.history['val_loss'].append(running_loss/len(val_loader))\n",
    "            self.history['val_acc'].append(100*correct/total)\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    \n",
    "    def __init__(self, args) -> None:\n",
    "        '''\n",
    "        Initializing the network parameters given arguments 'args'\n",
    "        See parse_args.py for further information on the available options.\n",
    "        '''\n",
    "\n",
    "        mnist_transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "        self.train_set = torchvision.datasets.MNIST(args.data_path, download=True, train=True, transform=mnist_transforms)\n",
    "        self.test_set = torchvision.datasets.MNIST(args.data_path, download=True, train=False, transform=mnist_transforms)\n",
    "\n",
    "        self.label_idxs = [np.array([], dtype=int) for i in range(10)]\n",
    "        for i, datapoint in enumerate(self.train_set):\n",
    "            self.label_idxs[datapoint[1]] = np.append(self.label_idxs[datapoint[1]], int(i))\n",
    "\n",
    "\n",
    "        # Initializing the global model\n",
    "        self.global_model = simpleModel()\n",
    "        self.global_model.load_state_dict(torch.load(os.path.join(args.saved_model_path, 'initial_model_weights.pth')))\n",
    "\n",
    "        # Initializing the clients\n",
    "        self.clients = []\n",
    "        for i in range(args.num_clients):\n",
    "            self.clients.append(MNIST_client(self.train_set, i, self.label_idxs, args))\n",
    "        \n",
    "        for client in self.clients:\n",
    "            print(len(client.dataset))\n",
    "        \n",
    "        \n",
    "            \n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_arguments = parse_arguments()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('mth555-fp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11b3f7d15bc931bd07ffc1df2d58cddf81ad17161371d834fccf8aa0d829f10b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
